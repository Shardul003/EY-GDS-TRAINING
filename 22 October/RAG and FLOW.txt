What Is RAG (Retrieval-Augmented Generation)?

Retrieval-Augmented Generation (RAG) is a powerful AI architecture that combines two main components:

Retrieval â†’ Searching for relevant information from external data sources (documents, databases, web, PDFs, etc.)

Generation â†’ Using a large language model (LLM) (like GPT) to create natural language answers based on the retrieved information.

ğŸ§  In short:

RAG = â€œSearch first, then generate.â€

It helps LLMs answer questions more accurately, using real-world, up-to-date, or private data, instead of relying only on what they were trained on.
What is the Purpose of RAG?  

Large language models (LLMs) like GPT and Gemini have been trained on large swaths of data from the internet but have some limitations.   

Knowledge cut-off: LLMs do not know anything that occurred after a certain cut-off date.   

Context window: LLMs can only â€œseeâ€ a certain number of tokens at a time.   

Hallucination risk: The model may sometimes â€œmake upâ€ facts when it is not sure if something is correct.  

RAG addresses these issues by connecting the LLM to some external sources of information, so the LLM can retrieve the fact for you.  

ğŸ§© The Main Building Blocks of a RAG System  

A typical RAG pipeline contains five main components:  

1ï¸âƒ£ Data Source / Knowledge Base  
This is where all of your reference information resides:  

Company documents (PDFs, reports, FAQs);  
Databases or APIs;  
Publicly available websites or history/tables from Wikipedia, etc.;  
CSV/Excel documents; etc.;  
These documents are pre-processed and stored to take up as little space as possible before you use them.  

2ï¸âƒ£ Chunking

Since LLMs cannot read and process full-length `long` documents all in once, we subdivision (or chunk) documents into smaller pieces, typically between 300-1000 words with slight overlapping.

For Example:

A Document: 10 pages 
â†“
Chunk 1: Page 1-2 
Chunk 2: Page 2-3 
Chunk 3: Page 3-4 
... 

The overlap page number ensures that no important context is lost between chunks.

3ï¸âƒ£ Embedding & Vector Store

Once the documents are chunked, you convert each of those "chunks" into an embedding which is a numerical representation of the meaning of the text in a vector.

ğŸ”¹ Example: 
â€œThe sun is brightâ€ â†’ [0.12, 0.56, 0.44, â€¦] 
â€œThe weather is sunnyâ€ â†’ [0.11, 0.54, 0.45, â€¦]

More generally, semantic meaning is assigned to words so that words with similar meanings will appear similar in the vector format.

You can store these vectors in a vector database which provides additional capabilities. Some examples of vector databases are:

FAISS (by Facebook)

Pinecone

Weaviate

Milvus

Chroma

An important capability of a vector database is to support semantic search which means you are finding documents with similar meaning, rather than just keywords.

4ï¸âƒ£ Retriever

When a user asks a question (this is the query step), RAG will convert the query into an embedding and then looks for document chunks in the vector database that are the most semantically similar. 

For Example: 
User: â€œWhat are Brillio's sustainability goals in 2024?â€ 
Retriever will search for chunks in the Brillio 2023 ESG report that are about sustainability goals. 

The retriever will typically return top-k relevant chunks (e.g., top 3-5).


ğŸ§  Example Scenario

Use case: An employee inquires with ChatGPT Enterprise:

â€œWhat is our companyâ€™s paid leave policy?â€

Steps:

The retriever searches the organizationâ€™s HR policy documents.

The retriever finds chunks of text related to â€œleave policyâ€ and â€œentitlements.â€

The LLM ingests those chunks and produces:

â€œBased on the HR Policy 2024, full-time employees are entitled to 18 days of earned leave per year, accrued on a monthly basis.â€

âœ… Accurate
âœ… Context-based
âœ… Grounded in real company data

ğŸ§® Types of RAG
1ï¸âƒ£ Simple RAG (Retrieve â†’ Generate)

Retrieve relevant text â†’ Feed to LLM â†’ Generate answer
This is the simplest iteration of RAG and most common.

2ï¸âƒ£ RAG with Re-ranking

Retrieve many chunks (e.g., top 20)

Using another model (a re-ranker), use the ranking of the chunks to reorder the chunksâ€™ relevance.

Only feed the best few chunks to the LLM.

3ï¸âƒ£ Iterative RAG

The LLM generates an intermediate query â†’ retrieves more data â†’ refines/refines answer.
Can be useful in multi-hop reasoning or complex question answering.

Benefits of RAG 

âœ… Enhances factual accuracy - based on retrieval of data
âœ… Tasks hallucination - the model can â€œquoteâ€ facts
âœ… Scale that knowledge - external Dbs can hold millions of documents
âœ… Customization - uses your private/domain-specific data
âœ… Fresh knowledge that can be indexed - just re-index the new data, and no retraining is required








âš ï¸ Challenges and Limitations

âŒ Requires good data preprocessing and chunking
âŒ Retrieval quality depends on embeddings & search tuning
âŒ High latency â€” two steps (retrieve + generate)
âŒ Cost â€” embedding & LLM usage both consume tokens
âŒ Complex pipeline â€” needs orchestration, monitoring, caching



What does the term "Flow" mean?

In relation to contemporary LLM tooling, the term "Flow" is more or less synonymous with orchestration / pipelines of prompts, models, and tools -- i.e. how you design multi-step interactions. The term may vary across different platforms â€” prompting flow, chains, pipelines, workflows. The general idea is to treat LLM reasoning as a directed sequence (or graph) of some operations that involve retrieval, LLM calls, tool calls, conditionals, loops, and external I/O.

Why do we favor flows?

It breaks the complex task into smaller debuggable components. Stepping through debugging a complex workflow is typically preferable to debugging one giant black box model call.

Logic: flows allow you to string multiple models/tools together e.g. a calculator, a DB, search, a planner, etc.

To add control flow: branches, retries, loops, steps in parallel.

Advisability, versioning and debugging steps.

 Flow Characteristics:

- Input & validation â€” the ability to sanitize and validate user inputs.
- Preprocessing â€” expand, normalize or convert queries, possible build in a call to a retriever.
- Decision logic / branching - e.g. Is this a simple fact question (use RAG) or a coding task (use code-model + execution sandbox)?
- LLM steps - one or a more than one prompt step of engagement (summarize â†’ extract â†’ compose).
- Tool calls - call external APIs (search, calculator, CRM).
- Postprocessing â€” format answer, consolidate citations, filtered answer.
- Recommendations & testing hooks â€” check output of steps; save inference traces.

  Flow types / patterns

Linear chain: step1 â†’ step2 â†’ step3 (basic pipelines).

Branching flow: if condition â†’ branch to one or a number of different flows.

Looping flow: ask follow-up question until user confidence desired threshold level is achieved.

Orchestrator + workers: main controller sends out sub-tasks (good to use for larger scale).

Plan-and-execute (agentic): planner step generates subgoals, agent performs step (tool use + retrieval).

Example cases of use

Multi-step QA: retrieve docs â†’ pull key facts â†’ synthesis answer with citations.

Complex form filling: extract user input â†’ validate entries â†’ fill out fields â†’ confirm.

Code + test generation: generating code â†’ run tests in sandbox â†’ if failed, ask LLM to debug â†’ repeat.

Customer support (chat-bot): classify intent â†’ retrieve KB â†’ answer, or escalate.






How RAG and Flow work together

A flow is the pipeline; RAG is one of the patterns/steps inside that pipeline.

Example flow: user query â†’ intent classifier â†’ (if factual) RAG retriever â†’ LLM synthesizer â†’ postprocess/cite.

Flows allow you to combine RAG with other capabilities (code execution, calculators, multi-turn memory) in a controlled way.
